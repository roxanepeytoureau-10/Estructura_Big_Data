version: '3.8'

services:
  # ---------------------------
  # Master node: NameNode + ResourceManager
  # ---------------------------
  master:
    image: my-hadoop:3.4.1
    container_name: hadoop-master
    hostname: master
    environment:
      - HDFS_NAMENODE=true
      - HDFS_NAMENODE_DIR=/hadoop/dfs/name
      - YARN_RESOURCEMANAGER=true
    ports:
      - "9870:9870"   # HDFS NameNode UI
      - "8088:8088"   # YARN ResourceManager UI
    volumes:
      - master_data:/hadoop/dfs/name
    networks:
      - hadoop-net
    command: >
      bash -c "
        yarn resourcemanager &
        hdfs namenode
      "
    env_file:
        - ./config

  # ---------------------------
  # Worker nodes: DataNode + NodeManager
  # ---------------------------
  worker1:
    image: my-hadoop:3.4.1
    container_name: hadoop-worker1
    hostname: worker1
    environment:
      - HDFS_DATANODE=true
      - YARN_NODEMANAGER=true
    volumes:
      - worker1_data:/hadoop/dfs/data
    networks:
      - hadoop-net
    depends_on:
      - master
    command: >
      bash -c "
        hdfs datanode &
        yarn nodemanager &
        tail -f /dev/null
      "
    env_file:
        - ./config

  worker2:
    image: my-hadoop:3.4.1
    container_name: hadoop-worker2
    hostname: worker2
    environment:
      - HDFS_DATANODE=true
      - YARN_NODEMANAGER=true
    volumes:
      - worker2_data:/hadoop/dfs/data
    networks:
      - hadoop-net
    depends_on:
      - master
    command: >
      bash -c "
        hdfs datanode &
        yarn nodemanager &
        tail -f /dev/null
      "
    env_file:
        - ./config

  worker3:
    image: my-hadoop:3.4.1
    container_name: hadoop-worker3
    hostname: worker3
    environment:
      - HDFS_DATANODE=true
      - YARN_NODEMANAGER=true
    volumes:
      - worker3_data:/hadoop/dfs/data
    networks:
      - hadoop-net
    depends_on:
      - master
    command: >
      bash -c "
        hdfs datanode &
        yarn nodemanager &
        tail -f /dev/null
      "
    env_file:
        - ./config

  # ---------------------------
  # Client node: submit Spark jobs
  # ---------------------------
  client:
    image: my-hadoop:3.4.1
    container_name: hadoop-client
    hostname: client
    networks:
      - hadoop-net
    volumes:
      - ./data:/tmp/data
    depends_on:
      - master
      - worker1
      - worker2
      - worker3
    command: tail -f /dev/null  # keep container alive
    env_file:
        - ./config

volumes:
  master_data:
  worker1_data:
  worker2_data:
  worker3_data:

networks:
  hadoop-net:
    driver: bridge
